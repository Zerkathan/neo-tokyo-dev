# âš¡ QUICK START - Neo-Tokyo Dev v3.0

## ğŸš€ De 0 a 100 en 5 minutos

### Para Usuarios que Quieren TODO GRATIS ğŸ†

```bash
# 1. Instalar Ollama
# Windows: Descargar de https://ollama.ai/download
# macOS: brew install ollama
# Linux: curl -fsSL https://ollama.ai/install.sh | sh

# 2. Descargar los 2 modelos del Golden Stack
ollama pull llama3.1
ollama pull qwen2.5-coder

# 3. Instalar dependencias Python
pip install -r requirements.txt

# 4. Â¡Listo! El .env ya estÃ¡ configurado
python ai_duo.py "Crear una funciÃ³n de validaciÃ³n de emails"
```

**Tiempo total:** ~10 minutos (dependiendo de tu internet para descargar ~9GB)

---

### Para Usuarios con API Keys ğŸ’³

```bash
# 1. Instalar dependencias
pip install -r requirements.txt

# 2. Editar .env y agregar tu API key
# Para Gemini:
REVIEW_PROVIDER=gemini
REVIEW_MODEL=gemini-1.5-pro
DEV_PROVIDER=gemini
DEV_MODEL=gemini-1.5-flash
GOOGLE_API_KEY=tu-api-key-aqui

# 3. Â¡Ejecutar!
python ai_duo.py "Tu problema aquÃ­"
```

**Tiempo total:** 2 minutos

---

## ğŸ§ª Primer Test

```bash
# Test simple
python ai_duo.py "Crear una funciÃ³n que calcule el factorial de un nÃºmero"

# Ver ejemplos mÃ¡s complejos
python test_example.py
```

---

## ğŸ“Š Â¿QuÃ© Stack Usar?

| Stack | Costo | Velocidad | Calidad | Mejor Para |
|-------|-------|-----------|---------|------------|
| ğŸ† **Golden Stack**<br>(Llama 3.1 + Qwen) | **$0** | âš¡âš¡âš¡ | â­â­â­â­ | Aprendizaje, proyectos personales |
| ğŸ’ **Claude Sonnet** | $$$ | âš¡âš¡ | â­â­â­â­â­ | ProducciÃ³n crÃ­tica |
| âš¡ **Gemini Flash** | $ | âš¡âš¡âš¡âš¡ | â­â­â­ | IteraciÃ³n rÃ¡pida |
| ğŸ¯ **HÃ­brido**<br>(Qwen + Claude) | $$ | âš¡âš¡âš¡ | â­â­â­â­â­ | Ã“ptimo precio/calidad |

---

## â“ FAQ RÃ¡pido

### Â¿Necesito GPU?
**No.** Los modelos del Golden Stack corren en CPU con 16GB RAM.

### Â¿Puedo mezclar local y cloud?
**SÃ­.** Usa Qwen local para cÃ³digo y Claude para arquitectura:
```env
DEV_PROVIDER=ollama
DEV_MODEL=qwen2.5-coder
REVIEW_PROVIDER=anthropic
REVIEW_MODEL=claude-sonnet-4-20250514
```

### Â¿QuÃ© tan bueno es Qwen vs GPT-4?
En **cÃ³digo puro**, Qwen 2.5 Coder (7B) estÃ¡ a ~5-10% de GPT-4, pero es:
- âœ… Gratis
- âœ… Local (privacidad)
- âœ… Sin rate limits

### Ollama no arranca
```bash
# Windows: Busca "Ollama" en el menÃº inicio
# macOS/Linux:
ollama serve
```

---

## ğŸ“ PrÃ³ximos Pasos

1. âœ… **Completar setup** (arriba)
2. ğŸ“– **Leer** [setup_golden_stack.md](setup_golden_stack.md) para detalles
3. ğŸ§ª **Probar** con `test_example.py`
4. ğŸš€ **Usar** en tus proyectos reales

---

**Â¿Problemas?** Abre un issue o consulta la [documentaciÃ³n completa](README.md).


# ğŸš€ Advanced Use Cases - Escalando la OperaciÃ³n

## ğŸ”® Neo-Tokyo Dev v3.0 - Casos de Uso Avanzados

Una vez dominaste el **Info-Harvester** (MisiÃ³n 2), es hora de apuntar tu "Taladro de Datos" a objetivos mÃ¡s valiosos.

---

## ğŸ“Š **CASO DE USO 3: Monitor de Precios (Arbitraje)**

### ğŸ¯ **Objetivo:**
Scraper que monitorea precios de productos (ej: tarjetas grÃ¡ficas RTX 4090) en mÃºltiples tiendas cada 60 segundos y alerta cuando el precio baja del promedio histÃ³rico.

### ğŸ—ï¸ **Arquitectura Propuesta:**

```python
# Estructura de datos
@dataclass
class PricePoint:
    product: str          # "RTX 4090"
    store: str           # "Amazon", "Newegg", "Best Buy"
    price: float         # 1599.99
    currency: str        # "USD"
    in_stock: bool       # True/False
    url: str             # Product URL
    timestamp: datetime  # When scraped
    
# Stores to monitor
STORES = {
    "Amazon": "https://www.amazon.com/s?k=rtx+4090",
    "Newegg": "https://www.newegg.com/p/pl?d=rtx+4090",
    "Best Buy": "https://www.bestbuy.com/site/searchpage.jsp?st=rtx+4090"
}
```

### ğŸ¨ **Features:**
- âœ… **Async monitoring** de 3+ tiendas simultÃ¡neamente
- âœ… **Price history** guardado en SQLite o JSON
- âœ… **Alert system** cuando precio < promedio_histÃ³rico - 5%
- âœ… **Rate limiting** (60 segundos entre scrapes)
- âœ… **User-Agent rotation** para evitar baneos
- âœ… **Discord/Telegram webhook** para alertas en tiempo real
- âœ… **CSV export** para anÃ¡lisis de datos
- âœ… **Web dashboard** (opcional) con grÃ¡ficos de tendencias

### ğŸ“ **Prompt para Neo-Tokyo Dev:**

```
ARQUITECTO (Temp 0.85):
DiseÃ±a un sistema de monitoreo de precios en tiempo real llamado "Price-Sentinel".

1. Arquitectura de datos:
   - Define una estructura unificada para precios de diferentes tiendas
   - DiseÃ±a un sistema de almacenamiento histÃ³rico (SQLite o JSON timestamped)
   - Crea un algoritmo de detecciÃ³n de "buenas ofertas" (precio < promedio - threshold)

2. Sistema de alertas:
   - Cuando detecte un precio bajo, debe notificar vÃ­a terminal (logs neon)
   - DiseÃ±a webhooks opcionales (Discord/Telegram) para alertas mÃ³viles

3. Rate limiting inteligente:
   - Scraping cada 60 segundos por tienda
   - SemÃ¡foros para no saturar la red
   - RotaciÃ³n de User-Agents

IMPLEMENTADOR (Temp 0.3):
Implementa Price-Sentinel en Python.

1. Stack obligatorio:
   - aiohttp + BeautifulSoup para scraping
   - asyncio para concurrencia
   - sqlite3 o JSON para almacenamiento
   - (Opcional) requests para webhooks

2. Funcionalidades:
   - async def monitor_store(store_name, url) -> PricePoint
   - def calculate_average(product, days=7) -> float
   - def detect_deal(current_price, avg_price, threshold=0.05) -> bool
   - async def send_alert(deal: PricePoint) -> None

3. Output:
   - Logs neon en tiempo real: "[PRICE] Amazon: $1,499 (â†“ 6% vs avg)"
   - Alertas cuando precio < promedio: "[DEAL!] RTX 4090 @ Amazon: $1,499 (save $100)"
   - Guardar en: output/price_history.json o price_sentinel.db
```

### ğŸ¬ **EjecuciÃ³n esperada:**

```bash
python ai_duo.py "..." # Pegar el prompt

# El DÃºo genera el cÃ³digo
# [SYSTEM] ğŸ’¾ Artifact secured: output/price_sentinel.py

python output/price_sentinel.py

# Output:
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”® PRICE-SENTINEL - Monitoring 3 stores...
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# [FETCH] Amazon...
# [FETCH] Newegg...
# [FETCH] Best Buy...
# [PRICE] Amazon: $1,599 (â†’ 0% vs avg)
# [PRICE] Newegg: $1,649 (+3% vs avg)
# [DEAL!] Best Buy: $1,499 (â†“ 6% vs avg) âš¡ ALERT SENT
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# [INFO] Next scan in 60 seconds...
```

---

## ğŸ’¼ **CASO DE USO 4: Buscador de Empleos (Job-Hunter)**

### ğŸ¯ **Objetivo:**
Scraper que busca ofertas de trabajo en LinkedIn, Indeed, y otros portales usando palabras clave especÃ­ficas ("Python", "Remote", "Cyberpunk", "AI Engineer") y guarda resultados en CSV con deduplicaciÃ³n.

### ğŸ—ï¸ **Arquitectura Propuesta:**

```python
# Estructura de datos
@dataclass
class JobListing:
    title: str           # "Senior Python Developer"
    company: str         # "OpenAI"
    location: str        # "Remote / San Francisco"
    salary: Optional[str] # "$150k - $200k" or None
    posted_date: str     # "2 days ago"
    url: str             # Job posting URL
    source: str          # "LinkedIn", "Indeed"
    keywords_matched: List[str]  # ["Python", "Remote"]
    timestamp: datetime  # When scraped
    
# Job boards to scrape
BOARDS = {
    "Indeed": "https://www.indeed.com/jobs?q={keywords}&l={location}",
    "LinkedIn": "https://www.linkedin.com/jobs/search?keywords={keywords}&location={location}",
    "RemoteOK": "https://remoteok.com/remote-{keywords}-jobs"
}

# Search config
SEARCH_CONFIG = {
    "keywords": ["Python", "AI Engineer", "Remote", "Cyberpunk"],
    "location": "Remote",
    "min_salary": 100000  # Filter jobs < $100k
}
```

### ğŸ¨ **Features:**
- âœ… **Multi-board scraping** (Indeed, LinkedIn, RemoteOK)
- âœ… **Keyword matching** con scoring (mÃ¡s keywords = mejor match)
- âœ… **Deduplication** (evita guardar el mismo job 2 veces)
- âœ… **Salary parsing** (extrae rangos salariales)
- âœ… **CSV export** para anÃ¡lisis en Excel/Google Sheets
- âœ… **Email alerts** cuando encuentre jobs con score > 80%
- âœ… **Cron scheduling** (ejecutar cada 6 horas automÃ¡ticamente)

### ğŸ“ **Prompt para Neo-Tokyo Dev:**

```
ARQUITECTO (Temp 0.85):
DiseÃ±a un sistema de bÃºsqueda de empleo automatizado llamado "Job-Hunter".

1. Arquitectura de datos:
   - Define estructura unificada JobListing para diferentes portales
   - Sistema de scoring: mÃ¡s keywords matched = mayor score (0-100)
   - DeduplicaciÃ³n: usar hash de (title + company) para evitar duplicados

2. Estrategia de scraping:
   - Scrape Indeed, LinkedIn, RemoteOK simultÃ¡neamente
   - Parsing inteligente de salarios ("$100k-$150k", "100000-150000", etc.)
   - Respeto de robots.txt y rate limiting

3. Alertas y export:
   - Guardar en CSV con columnas: title, company, location, salary, url, score, source
   - Alerta en terminal si score > 80%: "[MATCH!] Senior Python Dev @ OpenAI (95% match)"

IMPLEMENTADOR (Temp 0.3):
Implementa Job-Hunter en Python.

1. Stack:
   - aiohttp + BeautifulSoup (scraping)
   - csv module (export)
   - hashlib (deduplication)
   - re (salary parsing)

2. Funciones clave:
   - async def scrape_indeed(keywords, location) -> List[JobListing]
   - async def scrape_linkedin(keywords, location) -> List[JobListing]
   - def calculate_match_score(job: JobListing, search_config) -> int
   - def deduplicate_jobs(jobs: List[JobListing]) -> List[JobListing]
   - def export_to_csv(jobs: List[JobListing], filename: str)

3. Output:
   - Logs neon: "[FOUND] 15 jobs from Indeed, 23 from LinkedIn, 8 from RemoteOK"
   - CSV: output/job_listings_2025-12-05.csv
   - Alertas: "[MATCH!] AI Engineer @ DeepMind (92% match) - $180k-$220k"
```

### ğŸ¬ **EjecuciÃ³n esperada:**

```bash
python ai_duo.py "..." # Pegar el prompt

# El DÃºo genera el cÃ³digo
# [SYSTEM] ğŸ’¾ Artifact secured: output/job_hunter.py

python output/job_hunter.py

# Output:
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ğŸ”® JOB-HUNTER - Searching for: Python, AI Engineer, Remote
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# [FETCH] Indeed...
# [FETCH] LinkedIn...
# [FETCH] RemoteOK...
# [FOUND] Indeed: 15 jobs
# [FOUND] LinkedIn: 23 jobs
# [FOUND] RemoteOK: 8 jobs
# [FILTER] Removing duplicates... (3 duplicates found)
# [FILTER] Applying salary filter ($100k+)... (12 jobs passed)
# 
# [MATCH!] Senior Python Engineer @ OpenAI (95% match)
#   Location: Remote
#   Salary: $180k - $220k
#   URL: https://openai.com/careers/...
# 
# [MATCH!] AI Engineer @ DeepMind (88% match)
#   Location: London / Remote
#   Salary: Â£120k - Â£160k
#   URL: https://deepmind.google/careers/...
# 
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# [SUCCESS] 43 jobs saved to: output/job_listings_2025-12-05.csv
# [STATS] 2 high-match jobs found (score > 80%)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸŒ **OTROS CASOS DE USO AVANZADOS**

### ğŸ¢ **5. Real Estate Monitor**
Monitorear propiedades en Zillow/Redfin por precio, ubicaciÃ³n, y caracterÃ­sticas.

```python
# Alerta cuando:
# - Nueva propiedad < $500k en tu Ã¡rea
# - Precio reducido > 10%
# - Propiedad con palabras clave: "pool", "garage", "renovated"
```

### ğŸ“ˆ **6. Crypto Price Tracker**
Scraping de CoinMarketCap/CoinGecko para alertas de volatilidad.

```python
# Alerta cuando:
# - BTC sube/baja > 5% en 1 hora
# - Volumen de trading aumenta > 50%
# - Nuevas monedas listadas en exchanges top
```

### ğŸ“° **7. Research Paper Aggregator**
Scraping de arXiv/Google Scholar por papers recientes en tu campo.

```python
# Buscar papers con keywords:
# - "transformer", "large language models", "reinforcement learning"
# - Filtrar por citaciones > 100
# - Guardar PDFs automÃ¡ticamente
```

### ğŸª **8. Product Availability Monitor**
Alertas cuando productos agotados vuelven a stock (PS5, GPUs, etc.)

```python
# Monitorear:
# - Amazon, Best Buy, Newegg
# - Alerta cuando "Add to Cart" estÃ© disponible
# - Auto-checkout (avanzado)
```

---

## ğŸ¯ **CÃ“MO IMPLEMENTAR CUALQUIERA DE ESTOS**

### **Paso 1: Define tu objetivo**
```
"Necesito un monitor de precios para RTX 4090 en Amazon, Newegg, Best Buy"
```

### **Paso 2: Crea el Prompt Maestro**
```
Copia la plantilla de arriba y personaliza:
- Fuentes de datos (tiendas, job boards, etc.)
- Keywords y filtros
- Estructura de output (JSON, CSV, SQLite)
- Sistema de alertas (terminal, Discord, email)
```

### **Paso 3: Ejecuta Neo-Tokyo Dev**
```bash
echo "Monitor de precios" | python ai_duo.py "TU_PROMPT_MAESTRO"
```

### **Paso 4: El DÃºo trabaja**
```
Arquitecto diseÃ±a (5-10 min)
Implementador codifica (5-10 min)
Artifact Extraction guarda el cÃ³digo
```

### **Paso 5: Ejecuta y disfruta**
```bash
python output/price_sentinel.py
# o
python output/job_hunter.py
```

---

## ğŸ”® **FILOSOFÃA DE ESCALAMIENTO**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸš€ CUALQUIER CASO DE USO SIGUE EL MISMO PATRÃ“N:                     â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  1. Define estructura de datos unificada                             â•‘
â•‘  2. Async scraping con aiohttp + BeautifulSoup                       â•‘
â•‘  3. Rate limiting + User-Agent rotation                              â•‘
â•‘  4. Almacenamiento (JSON/CSV/SQLite)                                 â•‘
â•‘  5. Sistema de alertas (logs/webhooks/email)                         â•‘
â•‘  6. Logs neon cyberpunk                                              â•‘
â•‘  7. Artifact Extraction automÃ¡tico                                   â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

---

## ğŸ’¡ **PRO TIPS**

### **Para Web Scraping a Escala:**
1. **Respeta robots.txt** â†’ Usa `robotparser` o biblioteca `reppy`
2. **Rate limiting agresivo** â†’ MÃ­nimo 1-2 segundos entre requests
3. **User-Agent rotation** â†’ 5-10 agents diferentes
4. **Proxy rotation** (opcional) â†’ Para evitar IP bans
5. **Headless browsers** (Selenium/Playwright) â†’ Para sites con JS pesado
6. **API oficial primero** â†’ Siempre prefiere API oficial si existe

### **Para Datos Estructurados:**
1. **SQLite** â†’ Para historial y queries complejas
2. **CSV** â†’ Para anÃ¡lisis en Excel/Python/R
3. **JSON** â†’ Para integraciÃ³n con otras apps
4. **Pandas** â†’ Para anÃ¡lisis de datos avanzado

### **Para Alertas:**
1. **Terminal** â†’ Logs neon (rÃ¡pido, visual)
2. **Discord webhook** â†’ Alertas mÃ³viles (5 lÃ­neas de cÃ³digo)
3. **Telegram bot** â†’ Similar a Discord, mÃ¡s privado
4. **Email** â†’ SMTP para alertas profesionales
5. **Pushbullet** â†’ Notificaciones mÃ³viles directas

---

## ğŸŒŸ **CONCLUSIÃ“N**

El **Neo-Tokyo Dev v3.0** con **Artifact Extraction** te permite crear cualquier scraper/monitor en minutos, no horas.

**El lÃ­mite es tu imaginaciÃ³n.** ğŸ”®âš¡

---

**Generado por: Neo-Tokyo Dev v3.0 Golden Stack**  
**Architect (Llama 3.1 @ 0.85) + Implementer (Qwen 2.5 Coder @ 0.3)**


# ğŸ”® Intelligence Pipeline - Complete Guide

## ğŸ¯ **Overview**

Sistema completo de inteligencia automatizada con 3 componentes:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  INFO-HARVESTER â”‚  â†’   â”‚     ANALYST     â”‚  â†’   â”‚    WATCHDOG     â”‚
â”‚   (El Cuerpo)   â”‚      â”‚  (El Cerebro)   â”‚      â”‚   (El GuardiÃ¡n) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Scrapes data          Analyzes trends         Monitors & alerts
```

---

## ğŸš€ **Quick Start**

### **MÃ©todo 1: Pipeline Manual**

```bash
# Paso 1: Recolectar datos
python output/info_harvester.py
# Output: intelligence_report.json

# Paso 2: Analizar datos
python output/analyst.py
# Output: intelligence_summary.md

# Paso 3: Leer resumen
cat output/intelligence_summary.md
```

### **MÃ©todo 2: Pipeline Automatizada (Watchdog)**

```bash
# Single scan (testing)
python output/watchdog.py --once

# Continuous monitoring (production)
python output/watchdog.py
# Press Ctrl+C to stop
```

---

## ğŸ“¦ **Component 1: Info-Harvester**

### **Purpose:**
Scrape tech news from Hacker News and TechCrunch.

### **Features:**
- âœ… Async scraping (aiohttp)
- âœ… BeautifulSoup parsing
- âœ… User-Agent rotation
- âœ… Rate limiting
- âœ… JSON export

### **Usage:**
```bash
python output/info_harvester.py
```

### **Output:**
```json
{
  "metadata": {
    "harvested_at": "2025-12-05T02:31:36",
    "total_articles": 10,
    "sources": ["Hacker News", "TechCrunch"]
  },
  "articles": [
    {
      "title": "AV1: A Modern, Open Codec",
      "url": "https://...",
      "score": 268,
      "source": "Hacker News",
      "timestamp": "2025-12-05T02:31:36"
    }
  ]
}
```

### **Configuration:**
Edit `info_harvester.py`:
```python
# Add more sources
async def scrape_reddit(...):
    # Your implementation
```

---

## ğŸ“Š **Component 2: Analyst**

### **Purpose:**
Analyze intelligence_report.json to extract insights.

### **Features:**
- âœ… Keyword extraction
- âœ… Frequency analysis
- âœ… Thematic clustering
- âœ… Markdown report
- âœ… No external dependencies

### **Usage:**
```bash
python output/analyst.py
```

### **Output (Terminal):**
```
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š TOP 10 TRENDING TOPICS
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[ 1] ai                   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 4
[ 2] python               â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 3
[ 3] security             â–ˆâ–ˆâ–ˆâ–ˆ 2
```

### **Output (Markdown):**
```markdown
# ğŸ§  Intelligence Summary Report

## ğŸ”¥ Top 10 Trending Topics
1. **ai** `â–“â–“â–“â–“â–“â–“â–“â–“` (4 mentions)
2. **python** `â–“â–“â–“â–“â–“â–“` (3 mentions)

## ğŸ¯ Thematic Analysis
### ğŸ¤– AI & Machine Learning (5 mentions)
- **ai**: 4 mentions
- **machine**: 1 mention
```

### **Configuration:**
Edit `analyst.py`:
```python
# Add custom stopwords
STOPWORDS.add('custom_word')

# Add custom clusters
clusters["ğŸ”¥ Your Cluster"] = []
```

---

## ğŸ• **Component 3: Watchdog**

### **Purpose:**
Automated monitoring with keyword alerts.

### **Features:**
- âœ… Auto-execution of pipeline
- âœ… Keyword detection
- âœ… Sound alerts
- âœ… Desktop notifications (Windows)
- âœ… Configurable intervals
- âœ… Loop mode + single scan mode

### **Usage:**

**Testing:**
```bash
python output/watchdog.py --once
```

**Production (run in background):**
```bash
# Windows
start /B python output/watchdog.py

# Linux/Mac
nohup python output/watchdog.py &
```

### **Configuration:**
Edit `watchdog.py`:
```python
CONFIG = {
    # YOUR KEYWORDS HERE
    "watch_keywords": [
        "ai", "gpt", "llm",
        "rust", "python",
        "crypto", "bitcoin",
        "security", "vulnerability",
    ],
    
    # Minimum mentions to trigger alert
    "alert_threshold": 2,
    
    # Scan every X seconds
    "scan_interval": 300,  # 5 minutes
    
    # Enable/disable features
    "sound_alerts": True,
    "desktop_notifications": True,
}
```

### **Output:**
```
ğŸš¨ ALERT: 'ai' DETECTED! ğŸš¨
[ALERT] Found 3 mentions
[ALERT] Related articles:
  1. GPT-5 Released
  2. AI Regulation in EU
  3. Machine Learning Advances
[BEEP] ğŸ”Š
```

---

## ğŸ¯ **Use Cases**

### **1. Track AI Trends**
```python
# watchdog.py
CONFIG = {
    "watch_keywords": ["ai", "gpt", "claude", "gemini", "llm"],
    "alert_threshold": 1,
    "scan_interval": 180,  # 3 minutes
}
```

### **2. Monitor Your Tech Stack**
```python
CONFIG = {
    "watch_keywords": ["python", "fastapi", "postgresql", "react"],
    "alert_threshold": 2,
    "scan_interval": 600,  # 10 minutes
}
```

### **3. Security Watch**
```python
CONFIG = {
    "watch_keywords": [
        "vulnerability", "cve", "exploit", "breach",
        "security", "hack", "ransomware"
    ],
    "alert_threshold": 1,  # Alert immediately
    "scan_interval": 120,  # 2 minutes
}
```

### **4. Startup/Business Intelligence**
```python
CONFIG = {
    "watch_keywords": [
        "startup", "funding", "series", "acquisition",
        "ipo", "valuation", "unicorn"
    ],
    "alert_threshold": 2,
    "scan_interval": 300,
}
```

---

## ğŸ”§ **Customization**

### **Add New News Sources**

Edit `info_harvester.py`:
```python
async def scrape_reddit(session, semaphore):
    url = "https://reddit.com/r/programming"
    # Your scraping logic here
    return articles

# Add to main:
tasks = [
    fetch_hacker_news(...),
    fetch_techcrunch(...),
    scrape_reddit(...),  # NEW
]
```

### **Add Custom Thematic Clusters**

Edit `analyst.py`:
```python
cluster_keywords = {
    "ğŸ”¥ Your Custom Cluster": ['keyword1', 'keyword2'],
}
```

### **Add Email Alerts**

Edit `watchdog.py`:
```python
import smtplib

def send_email_alert(keyword, count):
    # Your SMTP logic
    pass

# In alert():
send_email_alert(keyword, count)
```

---

## ğŸ“Š **Performance**

### **Benchmarks:**
```
Info-Harvester:
  - 2 sources: ~5-10 seconds
  - 10 articles average

Analyst:
  - 10 articles: <1 second
  - No external dependencies

Watchdog:
  - Full pipeline: ~10-15 seconds
  - Memory: <50 MB
```

### **Scaling:**
```python
# For larger datasets:
# 1. Use async.gather with more sources
# 2. Implement caching
# 3. Use SQLite for historical data
# 4. Add rate limiting per source
```

---

## ğŸ› **Troubleshooting**

### **Problem: 403 Forbidden**
```
Solution: Website blocking requests
- Increase User-Agent rotation
- Add delays between requests
- Use proxies
- Consider headless browsers (Selenium)
```

### **Problem: No alerts triggered**
```
Solution: Lower threshold
CONFIG = {
    "alert_threshold": 1,  # Try 1 instead of 2
}
```

### **Problem: Too many alerts**
```
Solution: Increase threshold or interval
CONFIG = {
    "alert_threshold": 3,
    "scan_interval": 600,  # 10 minutes
}
```

---

## ğŸ“ **Learning Path**

### **Beginner:**
1. Run `info_harvester.py` manually
2. Read `intelligence_report.json`
3. Run `analyst.py` manually
4. Read `intelligence_summary.md`

### **Intermediate:**
1. Run `watchdog.py --once`
2. Modify `CONFIG` keywords
3. Test alerts

### **Advanced:**
1. Add new news sources
2. Customize thematic clusters
3. Implement email/Slack alerts
4. Create web dashboard
5. Store historical data in SQLite

---

## ğŸŒŸ **Best Practices**

### **1. Rate Limiting**
```python
# Respect robots.txt
# Add delays between requests
await asyncio.sleep(2)
```

### **2. Error Handling**
```python
try:
    result = await scrape_source()
except Exception as e:
    logger.error(f"Failed: {e}")
    continue  # Don't crash entire pipeline
```

### **3. Monitoring**
```python
# Log everything
# Track scan counts
# Monitor success rates
```

### **4. Data Retention**
```python
# Archive old reports
# Keep last 30 days
# Implement cleanup job
```

---

## ğŸš€ **Next Steps**

### **Enhancements:**
1. **Web Dashboard** - Flask/FastAPI UI
2. **Database** - SQLite for historical trends
3. **More Sources** - Reddit, Twitter, RSS feeds
4. **Sentiment Analysis** - NLP for article sentiment
5. **Telegram Bot** - Mobile alerts
6. **API** - Expose as REST API
7. **Docker** - Containerize pipeline
8. **Scheduling** - Cron/Task Scheduler integration

---

## ğŸ“š **Resources**

### **Dependencies:**
```bash
pip install aiohttp beautifulsoup4
```

### **Files:**
```
output/
â”œâ”€â”€ info_harvester.py     (300 lines)
â”œâ”€â”€ analyst.py            (300 lines)
â”œâ”€â”€ watchdog.py           (350 lines)
â”œâ”€â”€ intelligence_report.json
â””â”€â”€ intelligence_summary.md
```

### **GitHub:**
https://github.com/Zerkathan/neo-tokyo-dev

---

## ğŸŠ **Conclusion**

Tienes un sistema completo de inteligencia:
- ğŸ•·ï¸ Scrapes datos automÃ¡ticamente
- ğŸ§  Analiza tendencias
- ğŸ• Monitorea keywords
- ğŸš¨ Alerta en tiempo real

**Â¡La Matrix estÃ¡ bajo tu control!** ğŸ”®

---

**Powered by: Neo-Tokyo Dev v3.0 Golden Stack**  
**Generado con: Llama 3.1 + Qwen 2.5 Coder**


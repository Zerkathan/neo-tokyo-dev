# ğŸ† GOLDEN STACK SETUP - 100% GRATIS, NIVEL DIOS

## Â¿Por quÃ© esta combinaciÃ³n es superior?

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  ğŸ›ï¸ ARQUITECTO: Llama 3.1 (8B)  â”‚  âš¡ IMPLEMENTADOR: Qwen 2.5 Coder (7B)  â•‘
â•‘  âœ“ Razonamiento lÃ³gico          â”‚  âœ“ Supera a GPT-4 en cÃ³digo           â•‘
â•‘  âœ“ PlanificaciÃ³n estratÃ©gica    â”‚  âœ“ Sintaxis perfecta                  â•‘
â•‘  âœ“ Contexto de negocio          â”‚  âœ“ RefactorizaciÃ³n experta            â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### ğŸ¯ Ventajas del Golden Stack

1. **EspecializaciÃ³n**: No usas el mismo cerebro para todo
   - Llama 3.1 = El Gerente (entiende lÃ³gica humana y negocio)
   - Qwen Coder = El Ingeniero (entiende sintaxis y librerÃ­as)

2. **Velocidad**: Modelos pequeÃ±os (7B-8B parÃ¡metros)
   - Corren en laptops normales (16GB RAM)
   - No necesitas GPU gigante
   - Respuestas en segundos

3. **Costo**: **$0.00** - Cero API keys, cero lÃ­mites

---

## ğŸ“¦ PASO 1: Instalar Ollama

### Windows
1. Descarga Ollama desde: https://ollama.ai/download
2. Ejecuta el instalador
3. Verifica la instalaciÃ³n:
   ```powershell
   ollama --version
   ```

### macOS/Linux
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

---

## ğŸš€ PASO 2: Descargar los Modelos

Abre tu terminal y ejecuta estos comandos:

### 1. Arquitecto Supremo (Llama 3.1)
```bash
ollama pull llama3.1
```

**Info del modelo:**
- ParÃ¡metros: 8B
- TamaÃ±o: ~4.7 GB
- RAM necesaria: ~8 GB
- Especialidad: Razonamiento lÃ³gico y planificaciÃ³n

### 2. Implementador Supremo (Qwen 2.5 Coder)
```bash
ollama pull qwen2.5-coder
```

**Info del modelo:**
- ParÃ¡metros: 7B
- TamaÃ±o: ~4.3 GB
- RAM necesaria: ~8 GB
- Especialidad: CÃ³digo puro (supera GPT-4 en benchmarks)

### Verificar modelos instalados
```bash
ollama list
```

DeberÃ­as ver algo como:
```
NAME                ID              SIZE      MODIFIED
llama3.1:latest     abc123def       4.7 GB    2 minutes ago
qwen2.5-coder:latest xyz789ghi      4.3 GB    1 minute ago
```

---

## âš™ï¸ PASO 3: ConfiguraciÃ³n ya estÃ¡ lista

El archivo `.env` ya estÃ¡ configurado con:

```env
# ğŸ›ï¸ ARQUITECTO â†’ Llama 3.1
REVIEW_PROVIDER=ollama
REVIEW_MODEL=llama3.1

# âš¡ IMPLEMENTADOR â†’ Qwen 2.5 Coder
DEV_PROVIDER=ollama
DEV_MODEL=qwen2.5-coder

# ğŸ”Œ ConexiÃ³n local
LLAMA_BASE_URL=http://localhost:11434/v1
LLAMA_API_KEY=ollama
```

---

## ğŸ§ª PASO 4: Probar el Sistema

```bash
# Test bÃ¡sico
python ai_duo.py "Crear una funciÃ³n de validaciÃ³n de emails con regex"

# Test intermedio
python ai_duo.py "Implementar un sistema de cachÃ© LRU con complejidad O(1)"

# Test avanzado
python ai_duo.py "DiseÃ±ar una API REST con autenticaciÃ³n JWT y rate limiting"
```

---

## ğŸ“Š Benchmarks - Â¿Por quÃ© Qwen 2.5 Coder?

### HumanEval (Benchmark de cÃ³digo Python)

| Modelo | Score | Notas |
|--------|-------|-------|
| **Qwen 2.5 Coder 7B** | **61.5%** | ğŸ† Mejor en su categorÃ­a |
| GPT-4 (early) | 67.0% | MÃ¡s caro, no es local |
| Llama 3.1 8B | 48.0% | Mejor en razonamiento general |
| CodeLlama 7B | 45.5% | Especializado pero inferior |
| Gemini 1.5 Flash | ~52%* | Requiere API key |

*Estimado basado en benchmarks pÃºblicos

### MBPP (More Basic Python Problems)

| Modelo | Score |
|--------|-------|
| **Qwen 2.5 Coder 7B** | **70.2%** |
| GPT-4 | 75.0% |
| Llama 3.1 8B | 55.0% |

**ConclusiÃ³n**: Qwen 2.5 Coder a 7B estÃ¡ a solo 5-13 puntos de GPT-4, pero es:
- âœ… Gratis
- âœ… Local (privacidad total)
- âœ… Sin lÃ­mites de rate

---

## ğŸ”§ Troubleshooting

### Problema: "ollama: command not found"
**SoluciÃ³n**: 
- Windows: Reinicia la terminal despuÃ©s de instalar
- macOS/Linux: Ejecuta `source ~/.bashrc` o `source ~/.zshrc`

### Problema: "Connection refused at localhost:11434"
**SoluciÃ³n**: Ollama no estÃ¡ corriendo
```bash
# Windows (se inicia automÃ¡ticamente, pero si no):
# Busca "Ollama" en el menÃº inicio y Ã¡brelo

# macOS/Linux:
ollama serve
```

### Problema: Modelos muy lentos
**Posibles causas**:
1. RAM insuficiente (mÃ­nimo 16GB recomendado para ambos modelos)
2. Muchas aplicaciones abiertas
3. Modelo muy grande para tu hardware

**Soluciones**:
```bash
# Usa versiones cuantizadas (mÃ¡s rÃ¡pidas, menos precisiÃ³n):
ollama pull llama3.1:7b-instruct-q4_0
ollama pull qwen2.5-coder:7b-instruct-q4_0

# Actualiza el .env:
REVIEW_MODEL=llama3.1:7b-instruct-q4_0
DEV_MODEL=qwen2.5-coder:7b-instruct-q4_0
```

### Problema: Respuestas de baja calidad
**Nota**: Los modelos locales son muy buenos pero no mÃ¡gicos. Para mÃ¡xima calidad:
- Usa prompts mÃ¡s especÃ­ficos
- Da mÃ¡s contexto en el problema
- Considera usar Claude/GPT-4 para proyectos crÃ­ticos

---

## ğŸ’¡ Tips Pro

### 1. Mix & Match segÃºn necesidad
```env
# Para proyectos simples: Todo local
REVIEW_PROVIDER=ollama
DEV_PROVIDER=ollama

# Para proyectos crÃ­ticos: Arquitecto remoto, cÃ³digo local
REVIEW_PROVIDER=anthropic
REVIEW_MODEL=claude-sonnet-4-20250514
DEV_PROVIDER=ollama
DEV_MODEL=qwen2.5-coder
```

### 2. Monitorear uso de recursos
```bash
# Ver modelos cargados en memoria
ollama ps

# Si quieres liberar memoria
ollama stop llama3.1
ollama stop qwen2.5-coder
```

### 3. Actualizar modelos
```bash
# Los modelos se actualizan regularmente
ollama pull llama3.1  # actualiza si hay nueva versiÃ³n
ollama pull qwen2.5-coder
```

---

## ğŸ“ Recursos Adicionales

- [Ollama Documentation](https://github.com/ollama/ollama)
- [Qwen 2.5 Paper](https://arxiv.org/abs/2309.16609)
- [Llama 3.1 Blog Post](https://ai.meta.com/blog/meta-llama-3-1/)
- [HumanEval Benchmark](https://github.com/openai/human-eval)

---

## âœ… Checklist de Setup Completo

- [ ] Ollama instalado (`ollama --version`)
- [ ] Llama 3.1 descargado (`ollama list`)
- [ ] Qwen 2.5 Coder descargado (`ollama list`)
- [ ] Archivo `.env` configurado
- [ ] Ollama corriendo (`ollama ps`)
- [ ] Test ejecutado exitosamente

**Â¡Listo! Tienes un sistema de colaboraciÃ³n IA de nivel FAANG, completamente gratis.** ğŸ”®âš¡

